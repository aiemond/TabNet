{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e95f9e4",
   "metadata": {},
   "source": [
    "# Sequential data set with random missing values, size of the dataset (1000*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ab68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with Sequentially Increasing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0       0.2       0.4       0.6       0.8       1.0       1.2       1.4   \n",
      "1       2.2       2.4       2.6       2.8       3.0       3.2       3.4   \n",
      "2       4.2       4.4       4.6       4.8       5.0       5.2       5.4   \n",
      "3       6.2       6.4       6.6       6.8       7.0       7.2       7.4   \n",
      "4       8.2       8.4       8.6       8.8       9.0       9.2       9.4   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0       1.6       1.8        2.0  \n",
      "1       3.6       3.8        4.0  \n",
      "2       5.6       5.8        6.0  \n",
      "3       7.6       7.8        8.0  \n",
      "4       9.6       9.8       10.0  \n",
      "After Normalizing Input Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320   \n",
      "1 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856   \n",
      "2 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391   \n",
      "3 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391  -1.723391  \n",
      "3 -1.719927 -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463  -1.716463  \n",
      "\n",
      "Data with Missing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320       NaN       NaN       NaN   \n",
      "1       NaN -1.726856 -1.726856 -1.726856       NaN       NaN -1.726856   \n",
      "2 -1.723391       NaN -1.723391 -1.723391 -1.723391 -1.723391       NaN   \n",
      "3 -1.719927       NaN       NaN -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4       NaN -1.716463       NaN -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391        NaN  \n",
      "3       NaN -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463        NaN  \n",
      "\n",
      "Filled Missing Values with Placeholder:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320  0.000000  0.000000  0.000000   \n",
      "1  0.000000 -1.726856 -1.726856 -1.726856  0.000000  0.000000 -1.726856   \n",
      "2 -1.723391  0.000000 -1.723391 -1.723391 -1.723391 -1.723391  0.000000   \n",
      "3 -1.719927  0.000000  0.000000 -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4  0.000000 -1.716463  0.000000 -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391   0.000000  \n",
      "3  0.000000 -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463   0.000000  \n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.18323 |  0:00:00s\n",
      "epoch 1  | loss: 2.60761 |  0:00:01s\n",
      "epoch 2  | loss: 1.78214 |  0:00:02s\n",
      "epoch 3  | loss: 1.44374 |  0:00:03s\n",
      "epoch 4  | loss: 1.2396  |  0:00:04s\n",
      "epoch 5  | loss: 1.10012 |  0:00:05s\n",
      "epoch 6  | loss: 1.06249 |  0:00:05s\n",
      "epoch 7  | loss: 1.00249 |  0:00:07s\n",
      "epoch 8  | loss: 0.94905 |  0:00:08s\n",
      "epoch 9  | loss: 0.9234  |  0:00:08s\n",
      "epoch 10 | loss: 0.85127 |  0:00:08s\n",
      "epoch 11 | loss: 0.85337 |  0:00:09s\n",
      "epoch 12 | loss: 0.82432 |  0:00:09s\n",
      "epoch 13 | loss: 0.77418 |  0:00:10s\n",
      "epoch 14 | loss: 0.75897 |  0:00:11s\n",
      "epoch 15 | loss: 0.71873 |  0:00:11s\n",
      "epoch 16 | loss: 0.70344 |  0:00:12s\n",
      "epoch 17 | loss: 0.66833 |  0:00:12s\n",
      "epoch 18 | loss: 0.67987 |  0:00:13s\n",
      "epoch 19 | loss: 0.65375 |  0:00:14s\n",
      "epoch 20 | loss: 0.61257 |  0:00:15s\n",
      "epoch 21 | loss: 0.59387 |  0:00:17s\n",
      "epoch 22 | loss: 0.5868  |  0:00:19s\n",
      "epoch 23 | loss: 0.5904  |  0:00:20s\n",
      "epoch 24 | loss: 0.57496 |  0:00:22s\n",
      "epoch 25 | loss: 0.55331 |  0:00:22s\n",
      "epoch 26 | loss: 0.54171 |  0:00:24s\n",
      "epoch 27 | loss: 0.52697 |  0:00:25s\n",
      "epoch 28 | loss: 0.50677 |  0:00:26s\n",
      "epoch 29 | loss: 0.53679 |  0:00:28s\n",
      "epoch 30 | loss: 0.52789 |  0:00:29s\n",
      "epoch 31 | loss: 0.50741 |  0:00:31s\n",
      "epoch 32 | loss: 0.49321 |  0:00:32s\n",
      "epoch 33 | loss: 0.4661  |  0:00:34s\n",
      "epoch 34 | loss: 0.49218 |  0:00:35s\n",
      "epoch 35 | loss: 0.48815 |  0:00:36s\n",
      "epoch 36 | loss: 0.49673 |  0:00:38s\n",
      "epoch 37 | loss: 0.47547 |  0:00:39s\n",
      "epoch 38 | loss: 0.47551 |  0:00:40s\n",
      "epoch 39 | loss: 0.47054 |  0:00:40s\n",
      "epoch 40 | loss: 0.4548  |  0:00:41s\n",
      "epoch 41 | loss: 0.43842 |  0:00:42s\n",
      "epoch 42 | loss: 0.45073 |  0:00:43s\n",
      "epoch 43 | loss: 0.4827  |  0:00:44s\n",
      "epoch 44 | loss: 0.42487 |  0:00:46s\n",
      "epoch 45 | loss: 0.43231 |  0:00:49s\n",
      "epoch 46 | loss: 0.45627 |  0:00:50s\n",
      "epoch 47 | loss: 0.43349 |  0:00:52s\n",
      "epoch 48 | loss: 0.43185 |  0:00:53s\n",
      "epoch 49 | loss: 0.46788 |  0:00:55s\n",
      "\n",
      "Pretrained TabNet Model:\n",
      "\n",
      " True missing values: \n",
      "[[-1.73031962 -1.73031962 -1.73031962 ... -1.73031962 -1.73031962\n",
      "  -1.73031962]\n",
      " [ 0.         -1.72685552 -1.72685552 ... -1.72685552 -1.72685552\n",
      "  -1.72685552]\n",
      " [-1.72339142  0.         -1.72339142 ... -1.72339142 -1.72339142\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          1.72339142  1.72339142 ...  1.72339142  1.72339142\n",
      "   1.72339142]\n",
      " [ 1.72685552  1.72685552  1.72685552 ...  1.72685552  0.\n",
      "   1.72685552]\n",
      " [ 1.73031962  1.73031962  1.73031962 ...  1.73031962  1.73031962\n",
      "   1.73031962]]\n",
      "\n",
      " Reconstructed data: \n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.733663 -1.734009 -1.734356 -1.734702 -1.732052 -1.732398 -1.732744   \n",
      "1 -1.730666 -1.734003 -1.734350 -1.734696 -1.732052 -1.732398 -1.735735   \n",
      "2 -1.733651 -1.731012 -1.734344 -1.734690 -1.735037 -1.735383 -1.732744   \n",
      "3 -1.733645 -1.731012 -1.731359 -1.734684 -1.735031 -1.735377 -1.735723   \n",
      "4 -1.730666 -1.733985 -1.731359 -1.734678 -1.735025 -1.735371 -1.735717   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.736088 -1.736434  -1.736781  \n",
      "1 -1.736082 -1.736428  -1.736775  \n",
      "2 -1.736076 -1.736422  -1.733784  \n",
      "3 -1.733091 -1.736416  -1.736763  \n",
      "4 -1.736064 -1.736410  -1.733784  \n",
      "\n",
      " Imputed values: \n",
      "[[-1.73366304 -1.73400945 -1.73435586 ... -1.73608791 -1.73643432\n",
      "  -1.73678073]\n",
      " [-1.73066603 -1.73400345 -1.73434986 ... -1.73608191 -1.73642832\n",
      "  -1.73677473]\n",
      " [-1.73365104 -1.73101244 -1.73434386 ... -1.73607591 -1.73642232\n",
      "  -1.73378373]\n",
      " ...\n",
      " [-1.73066603 -1.72802744 -1.72837385 ... -1.7301059  -1.73045231\n",
      "  -1.73079872]\n",
      " [-1.72767503 -1.72802144 -1.72836785 ... -1.7300999  -1.73343731\n",
      "  -1.73079272]\n",
      " [-1.72766903 -1.72801544 -1.72836185 ... -1.7300939  -1.73044031\n",
      "  -1.73078672]]\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320  0.000000  0.000000  0.000000   \n",
      "1  0.000000 -1.726856 -1.726856 -1.726856  0.000000  0.000000 -1.726856   \n",
      "2 -1.723391  0.000000 -1.723391 -1.723391 -1.723391 -1.723391  0.000000   \n",
      "3 -1.719927  0.000000  0.000000 -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4  0.000000 -1.716463  0.000000 -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391   0.000000  \n",
      "3  0.000000 -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463   0.000000  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.733663 -1.734009 -1.734356 -1.734702 -1.732052 -1.732398 -1.732744   \n",
      "1 -1.730666 -1.734003 -1.734350 -1.734696 -1.732052 -1.732398 -1.735735   \n",
      "2 -1.733651 -1.731012 -1.734344 -1.734690 -1.735037 -1.735383 -1.732744   \n",
      "3 -1.733645 -1.731012 -1.731359 -1.734684 -1.735031 -1.735377 -1.735723   \n",
      "4 -1.730666 -1.733985 -1.731359 -1.734678 -1.735025 -1.735371 -1.735717   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.736088 -1.736434  -1.736781  \n",
      "1 -1.736082 -1.736428  -1.736775  \n",
      "2 -1.736076 -1.736422  -1.733784  \n",
      "3 -1.733091 -1.736416  -1.736763  \n",
      "4 -1.736064 -1.736410  -1.733784  \n",
      "MAE: 1.735613992941823\n",
      "R-squared: -3.787918995249095\n",
      "RMSE: 1.9504213334021807\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# Create a sequentially increasing dataset with multiple rows\n",
    "np.random.seed(42)\n",
    "num_samples = 1000  # Number of rows\n",
    "num_features = 10\n",
    "\n",
    "# Generate data with sequential increasing values by 0.5\n",
    "data = np.arange(1, (num_samples * num_features) + 1).reshape(num_samples, num_features) * 0.2\n",
    "\n",
    "# Convert the data array into a DataFrame\n",
    "input_data = pd.DataFrame(data, columns=[f'feature{i}' for i in range(1, num_features + 1)])\n",
    "\n",
    "# Print input dataset with sequentially increasing values\n",
    "print(\"Input Data with Sequentially Increasing Values:\")\n",
    "print(input_data.head())\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "omicMiss = pd.DataFrame(input_data)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in omicMiss.columns:\n",
    "    omicMiss[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "    \n",
    "#omicMiss = pd.DataFrame(data)\n",
    "print(\"\\nData with Missing Values:\")\n",
    "print(omicMiss.head())\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "print(\"\\nFilled Missing Values with Placeholder:\")\n",
    "print(omicMiss.head())\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "max_epochs = 50\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    pretraining_ratio=0.8,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "print(\"\\nPretrained TabNet Model:\")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "print ('\\n True missing values: ')\n",
    "print (true_missing_values)\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "print ('\\n Reconstructed data: ')\n",
    "print ( reconstructed_data.head())\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "print ('\\n Imputed values: ')\n",
    "print ( imputed_values)\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d650c2e",
   "metadata": {},
   "source": [
    "# Artifitially generated data with missing values, size of the dataset (1500*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db75e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with random missing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.496714  0.778361 -1.907808 -0.958778 -1.114081       NaN  0.765402   \n",
      "1 -0.138264 -0.551186 -0.860385       NaN -0.630931  0.121844  1.073413   \n",
      "2       NaN -0.818199 -0.413606 -1.583588 -0.942060  0.753417       NaN   \n",
      "3  1.523030 -0.003374  1.887688  0.412999 -0.547996  0.099826 -1.942498   \n",
      "4 -0.234153 -0.170185       NaN -0.214068 -0.214150 -0.667333 -0.155422   \n",
      "\n",
      "   feature8  feature9  feature10  feature11  feature12  feature13  feature14  \\\n",
      "0  1.541321       NaN   0.430846  -0.143423  -0.904591   0.321835   0.332621   \n",
      "1  1.333998       NaN   0.236542  -0.032656   0.932829  -0.781358        NaN   \n",
      "2       NaN -0.327795   0.767063        NaN  -0.695645   0.691356  -1.680780   \n",
      "3  0.074686 -0.041660   0.537984   0.946861  -0.940936  -0.590362   0.747610   \n",
      "4  0.022500  0.015909   0.717846  -0.747217   0.532116        NaN  -1.143671   \n",
      "\n",
      "   feature15 subtype  \n",
      "0   2.511133       B  \n",
      "1  -0.019932       B  \n",
      "2  -0.312567       C  \n",
      "3  -0.522360       C  \n",
      "4   0.376367       A  \n",
      "After Normalizing Input Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.459712  0.789061 -1.831991 -0.940697 -1.054812       NaN  0.793166   \n",
      "1 -0.189629 -0.576567 -0.804541       NaN -0.591672  0.096533  1.116130   \n",
      "2       NaN -0.850827 -0.366281 -1.565501 -0.889916  0.719429       NaN   \n",
      "3  1.509242 -0.013889  1.891130  0.431067 -0.512172  0.074817 -2.046194   \n",
      "4 -0.287687 -0.185227       NaN -0.195994 -0.192153 -0.681803 -0.172362   \n",
      "\n",
      "   feature8  feature9  feature10  feature11  feature12  feature13  feature14  \\\n",
      "0  1.588721       NaN   0.400899  -0.134934  -0.933856   0.317879   0.344425   \n",
      "1  1.378612       NaN   0.211268  -0.024005   0.920023  -0.788879        NaN   \n",
      "2       NaN -0.378520   0.729031        NaN  -0.723039   0.688595  -1.672052   \n",
      "3  0.102378 -0.086111   0.505461   0.956945  -0.970527  -0.597266   0.760049   \n",
      "4  0.049491 -0.027280   0.680998  -0.739611   0.515721        NaN  -1.134123   \n",
      "\n",
      "   feature15 subtype  \n",
      "0   2.556793       B  \n",
      "1   0.008943       B  \n",
      "2  -0.285633       C  \n",
      "3  -0.496817       C  \n",
      "4   0.407870       A  \n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.19577 |  0:00:01s\n",
      "epoch 1  | loss: 2.02667 |  0:00:01s\n",
      "epoch 2  | loss: 1.43769 |  0:00:03s\n",
      "epoch 3  | loss: 1.25731 |  0:00:04s\n",
      "epoch 4  | loss: 1.13006 |  0:00:06s\n",
      "epoch 5  | loss: 1.0798  |  0:00:07s\n",
      "epoch 6  | loss: 1.04473 |  0:00:09s\n",
      "epoch 7  | loss: 1.04118 |  0:00:11s\n",
      "epoch 8  | loss: 0.99826 |  0:00:12s\n",
      "epoch 9  | loss: 1.01083 |  0:00:14s\n",
      "epoch 10 | loss: 1.03067 |  0:00:15s\n",
      "epoch 11 | loss: 1.00692 |  0:00:17s\n",
      "epoch 12 | loss: 0.99443 |  0:00:18s\n",
      "epoch 13 | loss: 1.00248 |  0:00:19s\n",
      "epoch 14 | loss: 1.01161 |  0:00:21s\n",
      "epoch 15 | loss: 0.98417 |  0:00:22s\n",
      "epoch 16 | loss: 1.00374 |  0:00:23s\n",
      "epoch 17 | loss: 0.98504 |  0:00:25s\n",
      "epoch 18 | loss: 1.009   |  0:00:27s\n",
      "epoch 19 | loss: 0.99809 |  0:00:28s\n",
      "epoch 20 | loss: 0.99691 |  0:00:29s\n",
      "epoch 21 | loss: 0.9941  |  0:00:31s\n",
      "epoch 22 | loss: 1.00745 |  0:00:32s\n",
      "epoch 23 | loss: 1.0034  |  0:00:34s\n",
      "epoch 24 | loss: 0.99199 |  0:00:36s\n",
      "epoch 25 | loss: 0.98816 |  0:00:37s\n",
      "epoch 26 | loss: 1.00901 |  0:00:39s\n",
      "epoch 27 | loss: 0.99391 |  0:00:39s\n",
      "epoch 28 | loss: 0.96774 |  0:00:41s\n",
      "epoch 29 | loss: 1.0036  |  0:00:41s\n",
      "epoch 30 | loss: 1.01471 |  0:00:43s\n",
      "epoch 31 | loss: 0.97279 |  0:00:45s\n",
      "epoch 32 | loss: 1.00646 |  0:00:46s\n",
      "epoch 33 | loss: 1.01547 |  0:00:48s\n",
      "epoch 34 | loss: 0.99303 |  0:00:49s\n",
      "epoch 35 | loss: 0.99795 |  0:00:50s\n",
      "epoch 36 | loss: 0.99413 |  0:00:51s\n",
      "epoch 37 | loss: 0.99197 |  0:00:51s\n",
      "epoch 38 | loss: 0.98419 |  0:00:52s\n",
      "epoch 39 | loss: 1.00397 |  0:00:53s\n",
      "epoch 40 | loss: 1.01796 |  0:00:54s\n",
      "epoch 41 | loss: 0.99334 |  0:00:55s\n",
      "epoch 42 | loss: 0.9813  |  0:00:57s\n",
      "epoch 43 | loss: 0.99663 |  0:00:58s\n",
      "epoch 44 | loss: 0.99092 |  0:01:00s\n",
      "epoch 45 | loss: 1.01262 |  0:01:02s\n",
      "epoch 46 | loss: 1.01445 |  0:01:03s\n",
      "epoch 47 | loss: 1.00485 |  0:01:05s\n",
      "epoch 48 | loss: 0.998   |  0:01:06s\n",
      "epoch 49 | loss: 0.99089 |  0:01:08s\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.459712  0.789061 -1.831991 -0.940697 -1.054812  0.000000  0.793166   \n",
      "1 -0.189629 -0.576567 -0.804541  0.000000 -0.591672  0.096533  1.116130   \n",
      "2  0.000000 -0.850827 -0.366281 -1.565501 -0.889916  0.719429  0.000000   \n",
      "3  1.509242 -0.013889  1.891130  0.431067 -0.512172  0.074817 -2.046194   \n",
      "4 -0.287687 -0.185227  0.000000 -0.195994 -0.192153 -0.681803 -0.172362   \n",
      "\n",
      "   feature8  feature9  feature10  feature11  feature12  feature13  feature14  \\\n",
      "0  1.588721  0.000000   0.400899  -0.134934  -0.933856   0.317879   0.344425   \n",
      "1  1.378612  0.000000   0.211268  -0.024005   0.920023  -0.788879   0.000000   \n",
      "2  0.000000 -0.378520   0.729031   0.000000  -0.723039   0.688595  -1.672052   \n",
      "3  0.102378 -0.086111   0.505461   0.956945  -0.970527  -0.597266   0.760049   \n",
      "4  0.049491 -0.027280   0.680998  -0.739611   0.515721   0.000000  -1.134123   \n",
      "\n",
      "   feature15 subtype  \n",
      "0   2.556793       B  \n",
      "1   0.008943       B  \n",
      "2  -0.285633       C  \n",
      "3  -0.496817       C  \n",
      "4   0.407870       A  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.421873  0.800051 -1.757620 -0.922616 -0.997998 -0.023638  0.822279   \n",
      "1 -0.242155 -0.602638 -0.749762  0.018072 -0.554040  0.071568  1.160921   \n",
      "2 -0.048237 -0.884340 -0.319859 -1.547413 -0.839931  0.685907 -0.009394   \n",
      "3  1.495143 -0.024690  1.894506  0.449134 -0.477832  0.050151 -2.154925   \n",
      "4 -0.342431 -0.200677  0.039438 -0.177920 -0.171068 -0.696075 -0.190123   \n",
      "\n",
      "   feature8  feature9  feature10  feature11  feature12  feature13  feature14  \\\n",
      "0  1.636758 -0.043538   0.371673  -0.126433  -0.963384   0.313911   0.356248   \n",
      "1  1.423826 -0.043538   0.186601  -0.015341   0.907103  -0.796425   0.011296   \n",
      "2  0.026689 -0.430358   0.691915   0.008699  -0.750678   0.685824  -1.663311   \n",
      "3  0.130443 -0.131537   0.473720   0.967043  -1.000383  -0.604192   0.772507   \n",
      "4  0.076845 -0.071416   0.645037  -0.731993   0.499179  -0.004996  -1.124560   \n",
      "\n",
      "   feature15 subtype  \n",
      "0   2.602756       B  \n",
      "1   0.038009       B  \n",
      "2  -0.258520       C  \n",
      "3  -0.471105       C  \n",
      "4   0.439582       A  \n",
      "MAE: 0.025659596129374112\n",
      "R-squared: 0.9987121294660664\n",
      "RMSE: 0.03205986842549183\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "#dataset with missing values\n",
    "np.random.seed(42)\n",
    "num_samples = 1500\n",
    "num_features = 15\n",
    "data = {\n",
    "    f'feature{i}': np.random.normal(size=num_samples) for i in range(1, num_features + 1)\n",
    "}\n",
    "data['subtype'] = np.random.choice(['A', 'B', 'C'], size=num_samples)\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in data.keys():\n",
    "    if col != 'subtype':\n",
    "        data[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "\n",
    "omicMiss = pd.DataFrame(data)\n",
    "\n",
    "print(\"Input Data with random missing Values:\")\n",
    "print(omicMiss.head())\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "\n",
    "max_epochs = 50\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f018d44",
   "metadata": {},
   "source": [
    "# Data  from jiaojiao; size of the dataset (2700*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d902334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with missing Values:\n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                                NaN                           8.935561   \n",
      "1                           8.937573                           9.367173   \n",
      "2                           8.672674                           8.955487   \n",
      "3                                NaN                           8.596817   \n",
      "4                          11.804774                          11.778822   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                                NaN                                NaN   \n",
      "1                                NaN                                NaN   \n",
      "2                                NaN                                NaN   \n",
      "3                                NaN                                NaN   \n",
      "4                          11.864427                          12.075907   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                        9.220390                             NaN   \n",
      "1                        8.467625                        8.535367   \n",
      "2                        9.159973                        8.743835   \n",
      "3                             NaN                        8.679754   \n",
      "4                       11.444936                       11.350030   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                             NaN                        8.950183   \n",
      "1                        8.729995                             NaN   \n",
      "2                        9.052879                        8.759308   \n",
      "3                             NaN                        8.616568   \n",
      "4                       11.449346                       11.749893   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                            NaN                       9.239802  ...   \n",
      "1                       8.608477                       8.898448  ...   \n",
      "2                       8.201907                       9.012231  ...   \n",
      "3                       8.696092                       8.418146  ...   \n",
      "4                      10.275086                       9.828009  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                               NaN                          8.759982   \n",
      "1                          8.315028                          8.735943   \n",
      "2                          8.343125                          8.468654   \n",
      "3                          8.671355                          8.798077   \n",
      "4                         10.704165                         10.541333   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                       9.557964                       9.990720   \n",
      "1                       8.788090                       9.369052   \n",
      "2                       8.710834                       8.975048   \n",
      "3                       9.370757                       8.818201   \n",
      "4                      10.565428                      10.897665   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                       9.789927                      10.089345   \n",
      "1                       8.928256                       9.288782   \n",
      "2                       8.846468                       8.716585   \n",
      "3                       9.477080                       9.111724   \n",
      "4                      10.769873                      10.358631   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                           8.539033                           8.578702   \n",
      "1                           8.952269                           8.917941   \n",
      "2                           9.122460                           8.997914   \n",
      "3                           9.778208                           9.661671   \n",
      "4                           9.580386                           9.702839   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                           9.021260                           8.038641  \n",
      "1                           8.669605                           8.812814  \n",
      "2                           8.967428                           9.267760  \n",
      "3                           9.502039                           9.491149  \n",
      "4                           9.713053                          10.030737  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "After Normalizing Input Data:\n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                                NaN                          -0.994164   \n",
      "1                          -1.111167                          -0.660610   \n",
      "2                          -1.310759                          -0.978766   \n",
      "3                                NaN                          -1.255949   \n",
      "4                           1.049170                           1.203137   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                                NaN                                NaN   \n",
      "1                                NaN                                NaN   \n",
      "2                                NaN                                NaN   \n",
      "3                                NaN                                NaN   \n",
      "4                           0.990858                           1.231618   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                       -0.868289                             NaN   \n",
      "1                       -1.464529                       -1.394731   \n",
      "2                       -0.916143                       -1.230709   \n",
      "3                             NaN                       -1.281127   \n",
      "4                        0.893698                        0.819848   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                             NaN                       -1.024967   \n",
      "1                       -1.217255                             NaN   \n",
      "2                       -0.967875                       -1.173259   \n",
      "3                             NaN                       -1.284154   \n",
      "4                        0.883038                        1.150140   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                            NaN                      -0.794096  ...   \n",
      "1                      -1.292862                      -1.053465  ...   \n",
      "2                      -1.606033                      -0.967010  ...   \n",
      "3                      -1.225375                      -1.418408  ...   \n",
      "4                      -0.009117                      -0.347163  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                               NaN                         -1.258503   \n",
      "1                         -1.632974                         -1.277146   \n",
      "2                         -1.611296                         -1.484438   \n",
      "3                         -1.358050                         -1.228959   \n",
      "4                          0.210366                          0.122997   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                      -0.574717                      -0.200384   \n",
      "1                      -1.191714                      -0.700051   \n",
      "2                      -1.253630                      -1.016733   \n",
      "3                      -0.724750                      -1.142800   \n",
      "4                       0.232691                       0.528577   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                      -0.407933                      -0.134594   \n",
      "1                      -1.091332                      -0.795414   \n",
      "2                      -1.156198                      -1.267730   \n",
      "3                      -0.656055                      -0.941565   \n",
      "4                       0.369272                       0.087687   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                          -1.355760                          -1.329690   \n",
      "1                          -1.033380                          -1.060635   \n",
      "2                          -0.900608                          -0.997207   \n",
      "3                          -0.389035                          -0.470772   \n",
      "4                          -0.543363                          -0.438121   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                          -0.992154                          -1.768372  \n",
      "1                          -1.266527                          -1.163985  \n",
      "2                          -1.034156                          -0.808814  \n",
      "3                          -0.617034                          -0.634417  \n",
      "4                          -0.452394                          -0.213167  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      "Filled Missing Values with Placeholder:\n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                           0.000000                          -0.994164   \n",
      "1                          -1.111167                          -0.660610   \n",
      "2                          -1.310759                          -0.978766   \n",
      "3                           0.000000                          -1.255949   \n",
      "4                           1.049170                           1.203137   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                           0.000000                           0.000000   \n",
      "1                           0.000000                           0.000000   \n",
      "2                           0.000000                           0.000000   \n",
      "3                           0.000000                           0.000000   \n",
      "4                           0.990858                           1.231618   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                       -0.868289                        0.000000   \n",
      "1                       -1.464529                       -1.394731   \n",
      "2                       -0.916143                       -1.230709   \n",
      "3                        0.000000                       -1.281127   \n",
      "4                        0.893698                        0.819848   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                        0.000000                       -1.024967   \n",
      "1                       -1.217255                        0.000000   \n",
      "2                       -0.967875                       -1.173259   \n",
      "3                        0.000000                       -1.284154   \n",
      "4                        0.883038                        1.150140   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                       0.000000                      -0.794096  ...   \n",
      "1                      -1.292862                      -1.053465  ...   \n",
      "2                      -1.606033                      -0.967010  ...   \n",
      "3                      -1.225375                      -1.418408  ...   \n",
      "4                      -0.009117                      -0.347163  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                          0.000000                         -1.258503   \n",
      "1                         -1.632974                         -1.277146   \n",
      "2                         -1.611296                         -1.484438   \n",
      "3                         -1.358050                         -1.228959   \n",
      "4                          0.210366                          0.122997   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                      -0.574717                      -0.200384   \n",
      "1                      -1.191714                      -0.700051   \n",
      "2                      -1.253630                      -1.016733   \n",
      "3                      -0.724750                      -1.142800   \n",
      "4                       0.232691                       0.528577   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                      -0.407933                      -0.134594   \n",
      "1                      -1.091332                      -0.795414   \n",
      "2                      -1.156198                      -1.267730   \n",
      "3                      -0.656055                      -0.941565   \n",
      "4                       0.369272                       0.087687   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                          -1.355760                          -1.329690   \n",
      "1                          -1.033380                          -1.060635   \n",
      "2                          -0.900608                          -0.997207   \n",
      "3                          -0.389035                          -0.470772   \n",
      "4                          -0.543363                          -0.438121   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                          -0.992154                          -1.768372  \n",
      "1                          -1.266527                          -1.163985  \n",
      "2                          -1.034156                          -0.808814  \n",
      "3                          -0.617034                          -0.634417  \n",
      "4                          -0.452394                          -0.213167  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.42026 |  0:00:04s\n",
      "epoch 1  | loss: 1.87557 |  0:00:07s\n",
      "epoch 2  | loss: 1.34269 |  0:00:12s\n",
      "epoch 3  | loss: 0.96982 |  0:00:15s\n",
      "epoch 4  | loss: 0.75701 |  0:00:19s\n",
      "epoch 5  | loss: 0.59803 |  0:00:22s\n",
      "epoch 6  | loss: 0.49853 |  0:00:25s\n",
      "epoch 7  | loss: 0.41637 |  0:00:28s\n",
      "epoch 8  | loss: 0.38545 |  0:00:32s\n",
      "epoch 9  | loss: 0.32563 |  0:00:36s\n",
      "epoch 10 | loss: 0.31295 |  0:00:40s\n",
      "epoch 11 | loss: 0.29612 |  0:00:44s\n",
      "epoch 12 | loss: 0.28367 |  0:00:47s\n",
      "epoch 13 | loss: 0.28002 |  0:00:51s\n",
      "epoch 14 | loss: 0.26922 |  0:00:54s\n",
      "epoch 15 | loss: 0.26897 |  0:00:58s\n",
      "epoch 16 | loss: 0.264   |  0:01:01s\n",
      "epoch 17 | loss: 0.26044 |  0:01:03s\n",
      "epoch 18 | loss: 0.25265 |  0:01:06s\n",
      "epoch 19 | loss: 0.24837 |  0:01:08s\n",
      "epoch 20 | loss: 0.2421  |  0:01:11s\n",
      "epoch 21 | loss: 0.24939 |  0:01:13s\n",
      "epoch 22 | loss: 0.24509 |  0:01:16s\n",
      "epoch 23 | loss: 0.25262 |  0:01:19s\n",
      "epoch 24 | loss: 0.24624 |  0:01:21s\n",
      "epoch 25 | loss: 0.24534 |  0:01:24s\n",
      "epoch 26 | loss: 0.24398 |  0:01:28s\n",
      "epoch 27 | loss: 0.24528 |  0:01:33s\n",
      "epoch 28 | loss: 0.24543 |  0:01:37s\n",
      "epoch 29 | loss: 0.23604 |  0:01:43s\n",
      "epoch 30 | loss: 0.23631 |  0:01:48s\n",
      "epoch 31 | loss: 0.24049 |  0:01:54s\n",
      "epoch 32 | loss: 0.23729 |  0:02:00s\n",
      "epoch 33 | loss: 0.23873 |  0:02:03s\n",
      "epoch 34 | loss: 0.23386 |  0:02:06s\n",
      "epoch 35 | loss: 0.23341 |  0:02:11s\n",
      "epoch 36 | loss: 0.23737 |  0:02:15s\n",
      "epoch 37 | loss: 0.23454 |  0:02:19s\n",
      "epoch 38 | loss: 0.23115 |  0:02:22s\n",
      "epoch 39 | loss: 0.23795 |  0:02:27s\n",
      "epoch 40 | loss: 0.23577 |  0:02:32s\n",
      "epoch 41 | loss: 0.2325  |  0:02:36s\n",
      "epoch 42 | loss: 0.23169 |  0:02:40s\n",
      "epoch 43 | loss: 0.23614 |  0:02:45s\n",
      "epoch 44 | loss: 0.22929 |  0:02:49s\n",
      "epoch 45 | loss: 0.23047 |  0:02:55s\n",
      "epoch 46 | loss: 0.2287  |  0:03:01s\n",
      "epoch 47 | loss: 0.22511 |  0:03:06s\n",
      "epoch 48 | loss: 0.22399 |  0:03:11s\n",
      "epoch 49 | loss: 0.2244  |  0:03:16s\n",
      "\n",
      "Pretrained TabNet Model:\n",
      "\n",
      " True missing values: \n",
      "[[ 0.         -0.99416425  0.         ... -1.3296902  -0.99215424\n",
      "  -1.76837189]\n",
      " [-1.111167   -0.66061024  0.         ... -1.06063523 -1.26652729\n",
      "  -1.16398471]\n",
      " [-1.31075909 -0.97876565  0.         ... -0.99720694 -1.03415606\n",
      "  -0.8088137 ]\n",
      " ...\n",
      " [-1.3972859  -0.96934059 -1.30288959 ... -0.74731788 -0.68367893\n",
      "  -0.73294002]\n",
      " [ 0.         -2.48131956  0.         ... -1.75628181 -1.80571139\n",
      "   0.        ]\n",
      " [-1.06345986 -0.45089134 -1.20916969 ... -0.70624285 -0.97978186\n",
      "  -0.82069767]]\n",
      "\n",
      " Reconstructed data: \n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                           2.302907                          -0.994164   \n",
      "1                           2.690519                          -0.660610   \n",
      "2                           2.036815                          -0.978766   \n",
      "3                           2.131924                          -1.255949   \n",
      "4                           0.728017                           1.203137   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                          -1.670564                           1.299085   \n",
      "1                          -1.633412                           1.313071   \n",
      "2                          -1.895275                           1.749652   \n",
      "3                          -1.682524                           1.407282   \n",
      "4                          -1.517456                           0.440474   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                       -3.486395                        0.926928   \n",
      "1                       -3.264451                        0.902544   \n",
      "2                       -3.517904                        0.855611   \n",
      "3                       -3.476593                        0.809861   \n",
      "4                       -2.376795                       -0.766296   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                       -3.225537                       -1.168589   \n",
      "1                       -3.105226                       -1.061224   \n",
      "2                       -3.370568                       -1.189787   \n",
      "3                       -3.217000                       -1.143020   \n",
      "4                       -2.703811                       -1.814429   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                       1.353686                       1.905943  ...   \n",
      "1                       1.646007                       1.510316  ...   \n",
      "2                       1.266786                       2.288034  ...   \n",
      "3                       1.209956                       1.914858  ...   \n",
      "4                      -0.124912                       0.468903  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                          2.818697                         -1.205806   \n",
      "1                          2.850507                         -0.884490   \n",
      "2                          2.812544                         -1.366692   \n",
      "3                          2.649716                         -1.316375   \n",
      "4                         -0.137821                         -1.720999   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                      -0.271915                       6.409428   \n",
      "1                      -0.263485                       5.803863   \n",
      "2                      -0.149187                       6.706462   \n",
      "3                      -0.314464                       6.295686   \n",
      "4                      -0.360051                       2.275187   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                       2.398468                       4.259622   \n",
      "1                       1.780374                       4.184551   \n",
      "2                       2.609296                       4.298391   \n",
      "3                       2.266168                       4.038652   \n",
      "4                       0.328206                       2.144378   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                           3.061163                          -1.055492   \n",
      "1                           3.906107                          -0.964654   \n",
      "2                           2.602597                          -1.141721   \n",
      "3                           2.752311                          -1.089012   \n",
      "4                           1.478461                          -1.208511   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                          -2.668696                          -1.264143  \n",
      "1                          -2.511263                          -0.695261  \n",
      "2                          -2.997729                          -1.802291  \n",
      "3                          -2.798043                          -1.482437  \n",
      "4                          -2.012386                          -0.885897  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      " Imputed values: \n",
      "[[ 2.30290699 -0.99416425 -1.67056429 ... -1.05549216 -2.66869593\n",
      "  -1.26414323]\n",
      " [ 2.69051933 -0.66061024 -1.63341248 ... -0.96465409 -2.51126337\n",
      "  -0.69526088]\n",
      " [ 2.03681517 -0.97876565 -1.895275   ... -1.14172065 -2.99772882\n",
      "  -1.8022908 ]\n",
      " ...\n",
      " [ 2.20030737 -0.96934059 -1.55818164 ... -0.97498238 -2.617064\n",
      "  -1.22046685]\n",
      " [ 2.14954805 -2.48131956 -1.62365913 ... -0.73623192 -2.50066948\n",
      "  -1.04843926]\n",
      " [ 2.14356422 -0.45089134 -1.16046238 ... -0.72548723 -2.2501812\n",
      "  -0.72350758]]\n",
      "Original Data:\n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                           0.000000                          -0.994164   \n",
      "1                          -1.111167                          -0.660610   \n",
      "2                          -1.310759                          -0.978766   \n",
      "3                           0.000000                          -1.255949   \n",
      "4                           1.049170                           1.203137   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                           0.000000                           0.000000   \n",
      "1                           0.000000                           0.000000   \n",
      "2                           0.000000                           0.000000   \n",
      "3                           0.000000                           0.000000   \n",
      "4                           0.990858                           1.231618   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                       -0.868289                        0.000000   \n",
      "1                       -1.464529                       -1.394731   \n",
      "2                       -0.916143                       -1.230709   \n",
      "3                        0.000000                       -1.281127   \n",
      "4                        0.893698                        0.819848   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                        0.000000                       -1.024967   \n",
      "1                       -1.217255                        0.000000   \n",
      "2                       -0.967875                       -1.173259   \n",
      "3                        0.000000                       -1.284154   \n",
      "4                        0.883038                        1.150140   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                       0.000000                      -0.794096  ...   \n",
      "1                      -1.292862                      -1.053465  ...   \n",
      "2                      -1.606033                      -0.967010  ...   \n",
      "3                      -1.225375                      -1.418408  ...   \n",
      "4                      -0.009117                      -0.347163  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                          0.000000                         -1.258503   \n",
      "1                         -1.632974                         -1.277146   \n",
      "2                         -1.611296                         -1.484438   \n",
      "3                         -1.358050                         -1.228959   \n",
      "4                          0.210366                          0.122997   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                      -0.574717                      -0.200384   \n",
      "1                      -1.191714                      -0.700051   \n",
      "2                      -1.253630                      -1.016733   \n",
      "3                      -0.724750                      -1.142800   \n",
      "4                       0.232691                       0.528577   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                      -0.407933                      -0.134594   \n",
      "1                      -1.091332                      -0.795414   \n",
      "2                      -1.156198                      -1.267730   \n",
      "3                      -0.656055                      -0.941565   \n",
      "4                       0.369272                       0.087687   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                          -1.355760                          -1.329690   \n",
      "1                          -1.033380                          -1.060635   \n",
      "2                          -0.900608                          -0.997207   \n",
      "3                          -0.389035                          -0.470772   \n",
      "4                          -0.543363                          -0.438121   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                          -0.992154                          -1.768372  \n",
      "1                          -1.266527                          -1.163985  \n",
      "2                          -1.034156                          -0.808814  \n",
      "3                          -0.617034                          -0.634417  \n",
      "4                          -0.452394                          -0.213167  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      "Reconstructed Data:\n",
      "   Balm_3_1_U_IO_DDA_30min_G6_1_5228  Balm_3_2_T_IO_DDA_30min_H6_1_5230  \\\n",
      "0                           2.302907                          -0.994164   \n",
      "1                           2.690519                          -0.660610   \n",
      "2                           2.036815                          -0.978766   \n",
      "3                           2.131924                          -1.255949   \n",
      "4                           0.728017                           1.203137   \n",
      "\n",
      "   Balm_3_3_U_IO_DDA_30min_A7_1_5232  Balm_3_4_T_IO_DDA_30min_B7_1_5234  \\\n",
      "0                          -1.670564                           1.299085   \n",
      "1                          -1.633412                           1.313071   \n",
      "2                          -1.895275                           1.749652   \n",
      "3                          -1.682524                           1.407282   \n",
      "4                          -1.517456                           0.440474   \n",
      "\n",
      "   DOHH_2_1_U_IO_DDA_30_C4_1_5188  DOHH_2_2_T_IO_DDA_30_D4_1_5190  \\\n",
      "0                       -3.486395                        0.926928   \n",
      "1                       -3.264451                        0.902544   \n",
      "2                       -3.517904                        0.855611   \n",
      "3                       -3.476593                        0.809861   \n",
      "4                       -2.376795                       -0.766296   \n",
      "\n",
      "   DOHH_2_3_U_IO_DDA_30_E4_1_5192  DOHH_2_4_T_IO_DDA_30_F4_1_5194  \\\n",
      "0                       -3.225537                       -1.168589   \n",
      "1                       -3.105226                       -1.061224   \n",
      "2                       -3.370568                       -1.189787   \n",
      "3                       -3.217000                       -1.143020   \n",
      "4                       -2.703811                       -1.814429   \n",
      "\n",
      "   HBL_1_2_T_IO_DDA_30_H1_1_5150  HBL_1_3_U_IO_DDA_30_A2_1_5152  ...  \\\n",
      "0                       1.353686                       1.905943  ...   \n",
      "1                       1.646007                       1.510316  ...   \n",
      "2                       1.266786                       2.288034  ...   \n",
      "3                       1.209956                       1.914858  ...   \n",
      "4                      -0.124912                       0.468903  ...   \n",
      "\n",
      "   SU_DHL_5_3_U_IO_DDA_30_A3_1_5168  SU_DHL_5_4_T_IO_DDA_30_B3_1_5170  \\\n",
      "0                          2.818697                         -1.205806   \n",
      "1                          2.850507                         -0.884490   \n",
      "2                          2.812544                         -1.366692   \n",
      "3                          2.649716                         -1.316375   \n",
      "4                         -0.137821                         -1.720999   \n",
      "\n",
      "   TMD_8_1_U_IO_DDA_30_C2_1_5156  TMD_8_2_T_IO_DDA_30_D2_1_5158  \\\n",
      "0                      -0.271915                       6.409428   \n",
      "1                      -0.263485                       5.803863   \n",
      "2                      -0.149187                       6.706462   \n",
      "3                      -0.314464                       6.295686   \n",
      "4                      -0.360051                       2.275187   \n",
      "\n",
      "   TMD_8_3_U_IO_DDA_30_E2_1_5160  TMD_8_4_T_IO_DDA_30_F2_1_5162  \\\n",
      "0                       2.398468                       4.259622   \n",
      "1                       1.780374                       4.184551   \n",
      "2                       2.609296                       4.298391   \n",
      "3                       2.266168                       4.038652   \n",
      "4                       0.328206                       2.144378   \n",
      "\n",
      "   U_2932_1_U_IO_DDA_30min_G5_1_5212  U_2932_2_T_IO_DDA_30min_H5_1_5214  \\\n",
      "0                           3.061163                          -1.055492   \n",
      "1                           3.906107                          -0.964654   \n",
      "2                           2.602597                          -1.141721   \n",
      "3                           2.752311                          -1.089012   \n",
      "4                           1.478461                          -1.208511   \n",
      "\n",
      "   U_2932_3_U_IO_DDA_30min_A6_1_5216  U_2932_4_T_IO_DDA_30min_B6_1_5218  \n",
      "0                          -2.668696                          -1.264143  \n",
      "1                          -2.511263                          -0.695261  \n",
      "2                          -2.997729                          -1.802291  \n",
      "3                          -2.798043                          -1.482437  \n",
      "4                          -2.012386                          -0.885897  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "MAE: 1.8173416372692506\n",
      "R-squared: -5.003281017019347\n",
      "RMSE: 2.3651549206080604\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# dataset with missing values \n",
    "omicMiss = pd.read_csv('/Users/emondemoniac/Desktop/TabNet_PyTorch/jiaojiao data/48complete_proteomics.csv')\n",
    "omicMiss = omicMiss.iloc[:, 1:]\n",
    "omicMiss = pd.DataFrame(omicMiss)\n",
    "\n",
    "# Print input dataset \n",
    "print(\"Input Data with missing Values:\")\n",
    "print(omicMiss.head())\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = omicMiss.columns[0:]  # Assuming the first column is a non-numeric identifier\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "print(\"\\nFilled Missing Values with Placeholder:\")\n",
    "print(omicMiss.head())\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax',\n",
    "    n_d=32,  # Increase the number of decision steps\n",
    "    n_a=32   # Increase the number of features shared\n",
    ")\n",
    "\n",
    "max_epochs = 50\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "print(\"\\nPretrained TabNet Model:\")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "def tabnet_recon(omicMiss, network, omicMissMean=0, omicMissSd=1):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "    \n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "    \n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "    \n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "    \n",
    "    # Denormalize the reconstructed data\n",
    "    omicNa_tab = (results * omicMissSd) + omicMissMean\n",
    "    \n",
    "    # Combine reconstructed data with original non-numeric identifier column\n",
    "    omicNa_tab = pd.DataFrame(omicNa_tab, columns=numeric_cols)\n",
    "    omicNa_tab[omicMiss.columns[1]] = omicMiss[omicMiss.columns[1]]\n",
    "    \n",
    "    # Patch the reconstructed data into the original data with missing values\n",
    "    omicRec_tab = omicMiss.copy()\n",
    "    omicRec_tab.update(omicNa_tab)\n",
    "    \n",
    "    return omicRec_tab\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "print ('\\n True missing values: ')\n",
    "print (true_missing_values)\n",
    "\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "print ('\\n Reconstructed data: ')\n",
    "print ( reconstructed_data.head())\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "print ('\\n Imputed values: ')\n",
    "print ( imputed_values)\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "#omicMiss = omicMiss.iloc[:, 1:]\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "#reconstructed_data = reconstructed_data.iloc[:, 1:]\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781d2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
