{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bca51ad",
   "metadata": {},
   "source": [
    "# Artifitially generated data (1500*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b1729eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with random Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.496714  0.778361 -1.907808 -0.958778 -1.114081  1.725694  0.765402   \n",
      "1 -0.138264 -0.551186 -0.860385 -1.352509 -0.630931  0.121844  1.073413   \n",
      "2  0.647689 -0.818199 -0.413606 -1.583588 -0.942060  0.753417  0.498690   \n",
      "3  1.523030 -0.003374  1.887688  0.412999 -0.547996  0.099826 -1.942498   \n",
      "4 -0.234153 -0.170185  0.556553 -0.214068 -0.214150 -0.667333 -0.155422   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0  1.541321  1.174814   0.430846       A  \n",
      "1  1.333998 -1.878981   0.236542       B  \n",
      "2  0.777735 -0.327795   0.767063       C  \n",
      "3  0.074686 -0.041660   0.537984       C  \n",
      "4  0.022500  0.015909   0.717846       A  \n",
      "After Normalization:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.438938  0.800388       NaN -0.948301       NaN  1.671872  0.831004   \n",
      "1       NaN -0.558943 -0.793387 -1.341602       NaN       NaN  1.153698   \n",
      "2  0.593039 -0.831938 -0.356301 -1.572430 -0.907859  0.720301  0.551577   \n",
      "3       NaN  0.001140       NaN  0.421980       NaN  0.080628 -2.005984   \n",
      "4 -0.307065 -0.169407       NaN -0.204403 -0.193507 -0.670195 -0.133717   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0  1.595209  1.170724   0.389894       A  \n",
      "1  1.386964 -1.943422   0.199675       B  \n",
      "2  0.828229 -0.361581        NaN       C  \n",
      "3       NaN -0.069791   0.494779       C  \n",
      "4       NaN -0.011084   0.670860       A  \n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 2.96227 |  0:00:00s\n",
      "epoch 1  | loss: 1.70292 |  0:00:01s\n",
      "epoch 2  | loss: 1.32173 |  0:00:02s\n",
      "epoch 3  | loss: 1.21851 |  0:00:03s\n",
      "epoch 4  | loss: 1.13893 |  0:00:05s\n",
      "epoch 5  | loss: 1.10637 |  0:00:07s\n",
      "epoch 6  | loss: 1.02413 |  0:00:08s\n",
      "epoch 7  | loss: 1.00794 |  0:00:10s\n",
      "epoch 8  | loss: 1.01449 |  0:00:12s\n",
      "epoch 9  | loss: 1.03814 |  0:00:13s\n",
      "epoch 10 | loss: 1.01178 |  0:00:15s\n",
      "epoch 11 | loss: 1.02175 |  0:00:16s\n",
      "epoch 12 | loss: 1.01401 |  0:00:17s\n",
      "epoch 13 | loss: 1.00488 |  0:00:18s\n",
      "epoch 14 | loss: 1.02278 |  0:00:18s\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.438938  0.800388  0.000000 -0.948301  0.000000  1.671872  0.831004   \n",
      "1  0.000000 -0.558943 -0.793387 -1.341602  0.000000  0.000000  1.153698   \n",
      "2  0.593039 -0.831938 -0.356301 -1.572430 -0.907859  0.720301  0.551577   \n",
      "3  0.000000  0.001140  0.000000  0.421980  0.000000  0.080628 -2.005984   \n",
      "4 -0.307065 -0.169407  0.000000 -0.204403 -0.193507 -0.670195 -0.133717   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0  1.595209  1.170724   0.389894       A  \n",
      "1  1.386964 -1.943422   0.199675       B  \n",
      "2  0.828229 -0.361581   0.000000       C  \n",
      "3  0.000000 -0.069791   0.494779       C  \n",
      "4  0.000000 -0.011084   0.670860       A  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.379966  0.822907  0.048330 -0.937836  0.016655  1.619196  0.899733   \n",
      "1 -0.068062 -0.566875 -0.727842 -1.330708  0.016655 -0.017073  1.237810   \n",
      "2  0.537259 -0.845985 -0.300241 -1.561284 -0.874295  0.687889  0.606986   \n",
      "3 -0.068062  0.005755  0.048330  0.430952  0.016655  0.061838 -2.072497   \n",
      "4 -0.381486 -0.168612  0.048330 -0.194748 -0.173248 -0.672996 -0.110977   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0  1.649336  1.166554   0.349803       A  \n",
      "1  1.440166 -2.009137   0.163583       B  \n",
      "2  0.878947 -0.396034  -0.031894       C  \n",
      "3  0.047037 -0.098477   0.452483       C  \n",
      "4  0.047037 -0.038611   0.624862       A  \n",
      "MAE: 0.033309430162870496\n",
      "R-squared: 0.9979131456120224\n",
      "RMSE: 0.04094774644503755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# larger example dataset with missing values\n",
    "np.random.seed(42)\n",
    "num_samples = 1500\n",
    "num_features = 10\n",
    "data = {\n",
    "    f'feature{i}': np.random.normal(size=num_samples) for i in range(1, num_features + 1)\n",
    "}\n",
    "data['subtype'] = np.random.choice(['A', 'B', 'C'], size=num_samples)\n",
    "\n",
    "# Print input dataset with sequentially increasing values\n",
    "input_data = pd.DataFrame(data)\n",
    "print(\"Input Data with random Values:\")\n",
    "print(input_data.head())\n",
    "\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in data.keys():\n",
    "    if col != 'subtype':\n",
    "        data[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "\n",
    "omicMiss = pd.DataFrame(data)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalization:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "\n",
    "max_epochs = 15\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e85d74",
   "metadata": {},
   "source": [
    "# sequential increasing_subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "930a859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with Sequentially Increasing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0       1.0       2.0       3.0       4.0       5.0       6.0       7.0   \n",
      "1       2.0       3.0       4.0       5.0       6.0       7.0       8.0   \n",
      "2       3.0       4.0       5.0       6.0       7.0       8.0       9.0   \n",
      "3       4.0       5.0       6.0       7.0       8.0       9.0      10.0   \n",
      "4       5.0       6.0       7.0       8.0       9.0      10.0      11.0   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0       8.0       9.0       10.0       C  \n",
      "1       9.0      10.0       11.0       A  \n",
      "2      10.0      11.0       12.0       C  \n",
      "3      11.0      12.0       13.0       C  \n",
      "4      12.0      13.0       14.0       A  \n",
      "After Normalizing Input Data:\n",
      "      feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0    -1.730896 -1.730896 -1.730896 -1.730896 -1.730896 -1.730896 -1.730896   \n",
      "1    -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587   \n",
      "2    -1.726278 -1.726278 -1.726278 -1.726278 -1.726278 -1.726278 -1.726278   \n",
      "3    -1.723968 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968   \n",
      "4    -1.721659 -1.721659 -1.721659 -1.721659 -1.721659 -1.721659 -1.721659   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1495  1.721659  1.721659  1.721659  1.721659  1.721659  1.721659  1.721659   \n",
      "1496  1.723968  1.723968  1.723968  1.723968  1.723968  1.723968  1.723968   \n",
      "1497  1.726278  1.726278  1.726278  1.726278  1.726278  1.726278  1.726278   \n",
      "1498  1.728587  1.728587  1.728587  1.728587  1.728587  1.728587  1.728587   \n",
      "1499  1.730896  1.730896  1.730896  1.730896  1.730896  1.730896  1.730896   \n",
      "\n",
      "      feature8  feature9  feature10 subtype  \n",
      "0    -1.730896 -1.730896  -1.730896       C  \n",
      "1    -1.728587 -1.728587  -1.728587       A  \n",
      "2    -1.726278 -1.726278  -1.726278       C  \n",
      "3    -1.723968 -1.723968  -1.723968       C  \n",
      "4    -1.721659 -1.721659  -1.721659       A  \n",
      "...        ...       ...        ...     ...  \n",
      "1495  1.721659  1.721659   1.721659       B  \n",
      "1496  1.723968  1.723968   1.723968       B  \n",
      "1497  1.726278  1.726278   1.726278       B  \n",
      "1498  1.728587  1.728587   1.728587       C  \n",
      "1499  1.730896  1.730896   1.730896       A  \n",
      "\n",
      "[1500 rows x 11 columns]\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/g09gjq8d7n7bv5ky379fqnrm0000gn/T/ipykernel_55196/1731102013.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  omicMiss[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.84049 |  0:00:00s\n",
      "epoch 1  | loss: 1.52872 |  0:00:01s\n",
      "epoch 2  | loss: 1.05392 |  0:00:03s\n",
      "epoch 3  | loss: 0.93024 |  0:00:04s\n",
      "epoch 4  | loss: 0.81558 |  0:00:06s\n",
      "epoch 5  | loss: 0.70546 |  0:00:07s\n",
      "epoch 6  | loss: 0.63075 |  0:00:09s\n",
      "epoch 7  | loss: 0.57067 |  0:00:11s\n",
      "epoch 8  | loss: 0.50782 |  0:00:12s\n",
      "epoch 9  | loss: 0.44913 |  0:00:14s\n",
      "epoch 10 | loss: 0.41588 |  0:00:16s\n",
      "epoch 11 | loss: 0.41101 |  0:00:17s\n",
      "epoch 12 | loss: 0.35871 |  0:00:18s\n",
      "epoch 13 | loss: 0.35479 |  0:00:19s\n",
      "epoch 14 | loss: 0.35125 |  0:00:21s\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0  0.000000 -1.730896  0.000000 -1.730896 -1.730896  0.000000 -1.730896   \n",
      "1 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587   \n",
      "2 -1.726278  0.000000 -1.726278 -1.726278  0.000000 -1.726278 -1.726278   \n",
      "3 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968  0.000000 -1.723968   \n",
      "4 -1.721659 -1.721659 -1.721659  0.000000 -1.721659 -1.721659 -1.721659   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0 -1.730896 -1.730896  -1.730896       C  \n",
      "1 -1.728587  0.000000  -1.728587       A  \n",
      "2 -1.726278 -1.726278  -1.726278       C  \n",
      "3 -1.723968  0.000000   0.000000       C  \n",
      "4 -1.721659 -1.721659  -1.721659       A  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.733206 -1.739513 -1.737825 -1.744131 -1.746441 -1.744753 -1.751060   \n",
      "1 -1.737198 -1.739507 -1.741817 -1.744126 -1.746436 -1.748745 -1.751054   \n",
      "2 -1.737193 -1.735515 -1.741811 -1.744121 -1.742443 -1.748740 -1.751049   \n",
      "3 -1.737187 -1.739497 -1.741806 -1.744115 -1.746425 -1.744753 -1.751044   \n",
      "4 -1.737182 -1.739491 -1.741801 -1.740134 -1.746420 -1.748729 -1.751038   \n",
      "\n",
      "   feature8  feature9  feature10 subtype  \n",
      "0 -1.753369 -1.755678  -1.757988       C  \n",
      "1 -1.753364 -1.751681  -1.757983       A  \n",
      "2 -1.753358 -1.755668  -1.757977       C  \n",
      "3 -1.753353 -1.751681  -1.753991       C  \n",
      "4 -1.753348 -1.755657  -1.757967       A  \n",
      "MAE: 1.7421960212817151\n",
      "R-squared: -3.807501712527566\n",
      "RMSE: 1.9564759627753907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# Create a sequentially increasing dataset\n",
    "np.random.seed(42)\n",
    "num_samples = 1500\n",
    "num_features = 10\n",
    "\n",
    "data = {\n",
    "    f'feature{i}': np.linspace(i, i + num_samples - 1, num_samples) for i in range(1, num_features + 1)\n",
    "}\n",
    "data['subtype'] = np.random.choice(['A', 'B', 'C'], size=num_samples)\n",
    "\n",
    "\n",
    "# Print input dataset with sequentially increasing values\n",
    "input_data = pd.DataFrame(data)\n",
    "print(\"Input Data with Sequentially Increasing Values:\")\n",
    "print(input_data.head())\n",
    "\n",
    "# Create a DataFrame\n",
    "omicMiss = pd.DataFrame(input_data)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss)\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in data.keys():\n",
    "    if col != 'subtype':\n",
    "        omicMiss[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "max_epochs = 15\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82734d6e",
   "metadata": {},
   "source": [
    "# sequential increasing w/o subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "705d8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with Sequentially Increasing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0       1.0       2.0       3.0       4.0       5.0       6.0       7.0   \n",
      "1       2.0       3.0       4.0       5.0       6.0       7.0       8.0   \n",
      "2       3.0       4.0       5.0       6.0       7.0       8.0       9.0   \n",
      "3       4.0       5.0       6.0       7.0       8.0       9.0      10.0   \n",
      "4       5.0       6.0       7.0       8.0       9.0      10.0      11.0   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0       8.0       9.0       10.0  \n",
      "1       9.0      10.0       11.0  \n",
      "2      10.0      11.0       12.0  \n",
      "3      11.0      12.0       13.0  \n",
      "4      12.0      13.0       14.0  \n",
      "After Normalizing Input Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730896 -1.730896 -1.730896 -1.730896 -1.730896 -1.730896 -1.730896   \n",
      "1 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587 -1.728587   \n",
      "2 -1.726278 -1.726278 -1.726278 -1.726278 -1.726278 -1.726278 -1.726278   \n",
      "3 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968 -1.723968   \n",
      "4 -1.721659 -1.721659 -1.721659 -1.721659 -1.721659 -1.721659 -1.721659   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730896 -1.730896  -1.730896  \n",
      "1 -1.728587 -1.728587  -1.728587  \n",
      "2 -1.726278 -1.726278  -1.726278  \n",
      "3 -1.723968 -1.723968  -1.723968  \n",
      "4 -1.721659 -1.721659  -1.721659  \n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 2.81269 |  0:00:00s\n",
      "epoch 1  | loss: 1.56704 |  0:00:01s\n",
      "epoch 2  | loss: 1.14906 |  0:00:01s\n",
      "epoch 3  | loss: 0.9199  |  0:00:02s\n",
      "epoch 4  | loss: 0.79366 |  0:00:04s\n",
      "epoch 5  | loss: 0.74134 |  0:00:06s\n",
      "epoch 6  | loss: 0.65679 |  0:00:07s\n",
      "epoch 7  | loss: 0.59189 |  0:00:09s\n",
      "epoch 8  | loss: 0.53319 |  0:00:10s\n",
      "epoch 9  | loss: 0.49923 |  0:00:12s\n",
      "epoch 10 | loss: 0.45439 |  0:00:13s\n",
      "epoch 11 | loss: 0.42132 |  0:00:15s\n",
      "epoch 12 | loss: 0.39867 |  0:00:16s\n",
      "epoch 13 | loss: 0.3723  |  0:00:18s\n",
      "epoch 14 | loss: 0.37009 |  0:00:19s\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730896 -1.730896 -1.730896 -1.730896  0.000000  0.000000  0.000000   \n",
      "1  0.000000 -1.728587 -1.728587 -1.728587  0.000000  0.000000 -1.728587   \n",
      "2 -1.726278  0.000000 -1.726278 -1.726278 -1.726278 -1.726278  0.000000   \n",
      "3 -1.723968  0.000000  0.000000 -1.723968 -1.723968 -1.723968 -1.723968   \n",
      "4  0.000000 -1.721659  0.000000 -1.721659 -1.721659 -1.721659 -1.721659   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730896 -1.730896  -1.730896  \n",
      "1 -1.728587 -1.728587  -1.728587  \n",
      "2 -1.726278 -1.726278   0.000000  \n",
      "3  0.000000 -1.723968  -1.723968  \n",
      "4 -1.721659 -1.721659   0.000000  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.737203 -1.739513 -1.741822 -1.744131 -1.742443 -1.744753 -1.747062   \n",
      "1 -1.733206 -1.739507 -1.741817 -1.744126 -1.742443 -1.744753 -1.751054   \n",
      "2 -1.737193 -1.735515 -1.741811 -1.744121 -1.746430 -1.748740 -1.747062   \n",
      "3 -1.737187 -1.735515 -1.737825 -1.744115 -1.746425 -1.748734 -1.751044   \n",
      "4 -1.733206 -1.739491 -1.737825 -1.744110 -1.746420 -1.748729 -1.751038   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.753369 -1.755678  -1.757988  \n",
      "1 -1.753364 -1.755673  -1.757983  \n",
      "2 -1.753358 -1.755668  -1.753991  \n",
      "3 -1.749372 -1.755662  -1.757972  \n",
      "4 -1.753348 -1.755657  -1.753991  \n",
      "MAE: 1.745713411878661\n",
      "R-squared: -3.834586609973294\n",
      "RMSE: 1.9590164886894996\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# Create a sequentially increasing dataset\n",
    "np.random.seed(42)\n",
    "num_samples = 1500\n",
    "num_features = 10\n",
    "\n",
    "data = {\n",
    "    f'feature{i}': np.linspace(i, i + num_samples - 1, num_samples) for i in range(1, num_features + 1)\n",
    "}\n",
    "\n",
    "# Print input dataset with sequentially increasing values\n",
    "input_data = pd.DataFrame(data)\n",
    "print(\"Input Data with Sequentially Increasing Values:\")\n",
    "print(input_data.head())\n",
    "\n",
    "# Create a DataFrame\n",
    "omicMiss = pd.DataFrame(input_data)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in data.keys():\n",
    "    omicMiss[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "max_epochs = 15\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8516b",
   "metadata": {},
   "source": [
    "# sequential increasing rows by 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25a9caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data with Sequentially Increasing Values:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0       0.1       0.2       0.3       0.4       0.5       0.6       0.7   \n",
      "1       1.1       1.2       1.3       1.4       1.5       1.6       1.7   \n",
      "2       2.1       2.2       2.3       2.4       2.5       2.6       2.7   \n",
      "3       3.1       3.2       3.3       3.4       3.5       3.6       3.7   \n",
      "4       4.1       4.2       4.3       4.4       4.5       4.6       4.7   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0       0.8       0.9        1.0  \n",
      "1       1.8       1.9        2.0  \n",
      "2       2.8       2.9        3.0  \n",
      "3       3.8       3.9        4.0  \n",
      "4       4.8       4.9        5.0  \n",
      "After Normalizing Input Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320 -1.730320   \n",
      "1 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856 -1.726856   \n",
      "2 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391 -1.723391   \n",
      "3 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391  -1.723391  \n",
      "3 -1.719927 -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463  -1.716463  \n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 3.17779 |  0:00:01s\n",
      "epoch 1  | loss: 2.06687 |  0:00:02s\n",
      "epoch 2  | loss: 1.66521 |  0:00:03s\n",
      "epoch 3  | loss: 1.29724 |  0:00:04s\n",
      "epoch 4  | loss: 1.08698 |  0:00:05s\n",
      "epoch 5  | loss: 0.96713 |  0:00:06s\n",
      "epoch 6  | loss: 0.89712 |  0:00:08s\n",
      "epoch 7  | loss: 0.85591 |  0:00:10s\n",
      "epoch 8  | loss: 0.82094 |  0:00:11s\n",
      "epoch 9  | loss: 0.76484 |  0:00:12s\n",
      "epoch 10 | loss: 0.73842 |  0:00:14s\n",
      "epoch 11 | loss: 0.70252 |  0:00:15s\n",
      "epoch 12 | loss: 0.662   |  0:00:15s\n",
      "epoch 13 | loss: 0.61231 |  0:00:16s\n",
      "epoch 14 | loss: 0.55419 |  0:00:17s\n",
      "epoch 15 | loss: 0.5432  |  0:00:19s\n",
      "epoch 16 | loss: 0.50552 |  0:00:20s\n",
      "epoch 17 | loss: 0.50356 |  0:00:21s\n",
      "epoch 18 | loss: 0.48482 |  0:00:22s\n",
      "epoch 19 | loss: 0.44703 |  0:00:24s\n",
      "epoch 20 | loss: 0.45043 |  0:00:25s\n",
      "epoch 21 | loss: 0.4052  |  0:00:26s\n",
      "epoch 22 | loss: 0.41544 |  0:00:28s\n",
      "epoch 23 | loss: 0.40785 |  0:00:29s\n",
      "epoch 24 | loss: 0.39224 |  0:00:31s\n",
      "epoch 25 | loss: 0.37747 |  0:00:32s\n",
      "epoch 26 | loss: 0.37825 |  0:00:33s\n",
      "epoch 27 | loss: 0.3533  |  0:00:34s\n",
      "epoch 28 | loss: 0.36127 |  0:00:35s\n",
      "epoch 29 | loss: 0.35505 |  0:00:37s\n",
      "epoch 30 | loss: 0.35067 |  0:00:38s\n",
      "epoch 31 | loss: 0.35057 |  0:00:39s\n",
      "epoch 32 | loss: 0.33814 |  0:00:41s\n",
      "epoch 33 | loss: 0.33423 |  0:00:42s\n",
      "epoch 34 | loss: 0.33237 |  0:00:44s\n",
      "epoch 35 | loss: 0.3234  |  0:00:45s\n",
      "epoch 36 | loss: 0.31488 |  0:00:46s\n",
      "epoch 37 | loss: 0.31022 |  0:00:47s\n",
      "epoch 38 | loss: 0.31125 |  0:00:49s\n",
      "epoch 39 | loss: 0.31202 |  0:00:49s\n",
      "epoch 40 | loss: 0.3032  |  0:00:50s\n",
      "epoch 41 | loss: 0.28561 |  0:00:51s\n",
      "epoch 42 | loss: 0.30007 |  0:00:52s\n",
      "epoch 43 | loss: 0.29871 |  0:00:54s\n",
      "epoch 44 | loss: 0.30285 |  0:00:55s\n",
      "epoch 45 | loss: 0.28169 |  0:00:56s\n",
      "epoch 46 | loss: 0.29377 |  0:00:57s\n",
      "epoch 47 | loss: 0.2818  |  0:00:58s\n",
      "epoch 48 | loss: 0.28349 |  0:00:59s\n",
      "epoch 49 | loss: 0.28734 |  0:01:00s\n",
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.730320 -1.730320 -1.730320 -1.730320  0.000000  0.000000  0.000000   \n",
      "1  0.000000 -1.726856 -1.726856 -1.726856  0.000000  0.000000 -1.726856   \n",
      "2 -1.723391  0.000000 -1.723391 -1.723391 -1.723391 -1.723391  0.000000   \n",
      "3 -1.719927  0.000000  0.000000 -1.719927 -1.719927 -1.719927 -1.719927   \n",
      "4  0.000000 -1.716463  0.000000 -1.716463 -1.716463 -1.716463 -1.716463   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.730320 -1.730320  -1.730320  \n",
      "1 -1.726856 -1.726856  -1.726856  \n",
      "2 -1.723391 -1.723391   0.000000  \n",
      "3  0.000000 -1.719927  -1.719927  \n",
      "4 -1.716463 -1.716463   0.000000  \n",
      "\n",
      "Reconstructed Data:\n",
      "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
      "0 -1.736660 -1.737006 -1.737353 -1.737699 -1.732052 -1.732398 -1.732744   \n",
      "1 -1.730666 -1.736994 -1.737341 -1.737687 -1.732052 -1.732398 -1.738727   \n",
      "2 -1.736636 -1.731012 -1.737329 -1.737675 -1.738022 -1.738368 -1.732744   \n",
      "3 -1.736624 -1.731012 -1.731359 -1.737663 -1.738010 -1.738356 -1.738703   \n",
      "4 -1.730666 -1.736958 -1.731359 -1.737651 -1.737998 -1.738344 -1.738691   \n",
      "\n",
      "   feature8  feature9  feature10  \n",
      "0 -1.739085 -1.739431  -1.739778  \n",
      "1 -1.739073 -1.739419  -1.739766  \n",
      "2 -1.739061 -1.739407  -1.733784  \n",
      "3 -1.733091 -1.739395  -1.739742  \n",
      "4 -1.739037 -1.739383  -1.733784  \n",
      "MAE: 1.7356081126359426\n",
      "R-squared: -3.784438186576974\n",
      "RMSE: 1.9497122280630212\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# Create a sequentially increasing dataset with multiple rows\n",
    "np.random.seed(42)\n",
    "num_samples = 1000  # Number of rows\n",
    "num_features = 10\n",
    "\n",
    "# Generate data with sequential increasing values by 0.5\n",
    "data = np.arange(1, (num_samples * num_features) + 1).reshape(num_samples, num_features) * 0.1\n",
    "\n",
    "# Convert the data array into a DataFrame\n",
    "input_data = pd.DataFrame(data, columns=[f'feature{i}' for i in range(1, num_features + 1)])\n",
    "\n",
    "# Print input dataset with sequentially increasing values\n",
    "print(\"Input Data with Sequentially Increasing Values:\")\n",
    "print(input_data.head())\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "omicMiss = pd.DataFrame(input_data)\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [f'feature{i}' for i in range(1, num_features + 1)]\n",
    "omicMiss[numeric_cols] = scaler.fit_transform(omicMiss[numeric_cols])\n",
    "\n",
    "print ('After Normalizing Input Data:')\n",
    "print (omicMiss.head())\n",
    "\n",
    "# Introduce random missing values\n",
    "missing_mask = np.random.rand(num_samples, num_features) < 0.2\n",
    "for col in omicMiss.columns:\n",
    "    omicMiss[col][missing_mask[:, int(col[-1]) - 1]] = np.nan\n",
    "\n",
    "\n",
    "# Handle missing values by filling NaNs with a specific value\n",
    "missing_value_placeholder = 0\n",
    "omicMiss.fillna(missing_value_placeholder, inplace=True)\n",
    "\n",
    "# Pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "max_epochs = 50\n",
    "pretrained_model.fit(\n",
    "    omicMiss[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "# Define the tabnet_recon function\n",
    "def tabnet_recon(omicMiss, network):\n",
    "    omicMissTrain = omicMiss.copy()\n",
    "\n",
    "    # Normalize the missing data\n",
    "    omicMissTrain[numeric_cols] = scaler.transform(omicMissTrain[numeric_cols])\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    missing_mask = omicMissTrain[numeric_cols].isnull().values\n",
    "\n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(omicMissTrain[numeric_cols].values, dtype=torch.float32)\n",
    "\n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "\n",
    "    # Handle potential tuple output\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "\n",
    "    # If there are no missing values, return the original DataFrame\n",
    "    if np.sum(missing_mask) == 0:\n",
    "        return omicMissTrain\n",
    "\n",
    "    # Denormalize and reconstruct the missing values\n",
    "    imputed_values = results[missing_mask] * scaler.scale_ + scaler.mean_\n",
    "\n",
    "    # Replace missing values with imputed values\n",
    "    omicMissTrain.loc[missing_mask, numeric_cols] = imputed_values\n",
    "\n",
    "    return omicMissTrain\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = omicMiss[numeric_cols].values\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(omicMiss, network=pretrained_model)\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "# Calculate MAE, R-squared, and RMSE\n",
    "mae = np.mean(np.abs(imputed_values - true_missing_values))\n",
    "total_variation = np.sum((true_missing_values - np.mean(true_missing_values)) ** 2)\n",
    "residual_variation = np.sum((true_missing_values - imputed_values) ** 2)\n",
    "r_squared = 1 - (residual_variation / total_variation)\n",
    "rmse = np.sqrt(np.mean((imputed_values - true_missing_values) ** 2))\n",
    "\n",
    "# Print original and reconstructed data (only printing a subset)\n",
    "print(\"Original Data:\")\n",
    "print(omicMiss.head())\n",
    "print(\"\\nReconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805e06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
